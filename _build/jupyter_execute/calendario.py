#!/usr/bin/env python
# coding: utf-8

# # Calendario 
# 
# Viene qui fornito un calendario delle attività didattiche: l'elenco dei materiali da studiare ogni settimana, le scadenze per la consegna degli esercizi WarmUp, e le scadenze dei report in itinere. I compiti WarmUp assegnati vanno svolti **prima** di ciascuna lezione lezione. 
# 
# ```{warning}
# Il presente calendario **non è definitivo** e potrà subire delle variazioni che saranno comunicate mediante l'aggiornamento di questa pagina web.
# ```
# 
# ## Calendario
# 
# | Lezione | Data       | Argomento                                  | Materiali didattici     | Homework                 |
# | ------- | ---------- | ------------------------------------------ | ----------------------- | ------------------------ |
# | 1       | 2023/03/02 | Presentazione                       | [1][l1], [2][l2]  | -- | <!--intro to coding -->
# | 2       | 2023/03/06 | Introduzione a Python               | [3][l3]          | [hw1][hw1] |
# | 3       | 2023/03/07 | Introduzione a Numpy e Pandas       | [4][l4], [5][l5] | [hw2][hw2]; altri esercizi su Numpy sono forniti nella pagina web dedicata della dispensa |
# | 4       | 2023/03/09 | Introduzione a Matlibplot e Seaborn | [6][l6], [7][l7] | [hw3][hw3] | 
# | 5       | 2023/03/13 | La misurazione in psicologia; dati e frequenze | [8][l8], [9][l9], [10][l10] | [hw4][hw1] |
# | 6       | 2023/03/14 | Statistica descrittiva: indici di posizione e di scala; | [11][l11]  | [hw5][hw1] |
# | 7       | 2023/03/16 | Le relazioni tra variabili  | [12][l12]  | TODO | <!-- probability theory -->
# | 8       | 2023/03/20 | Calcolo combinatorio; probabilità condizionata | [13][l13], [14][l14], [15][l15] | [hw1][hw1] |
# | 9       | 2023/03/21 | Teorema di Bayes | [16][l16] | [hw1][hw1] |
# | 10       | 2023/03/23 | Variabili casuali, probabilità congiunta, densità di probabilità | [17][l17], [18][l18], [19][l19] | [hw1][hw1] |
# | 11       | 2023/03/27 | Distribuzioni di massa di probabilità     | [20][l20]    | TODO |
# | 12       | 2023/03/28 | Distribuzioni di densità di probabilità   | [21][l21]    | TODO |
# | 13      | 2023/03/30 | La funzione di verosimiglianza | [22][l22], [23][l23] | TODO |<!-- bayesiann inference -->
# | 14      | 2023/04/03 | Pensare alla probabilità in termini bayesiani | [24][l24], [25][l25] | TODO               |
# | 15      | 2023/04/04 | Distribuzioni coniugate; influenza della distribuzione a priori | [26][l26], [27][l27] | TODO |
# | 16      | 2023/04/06 | Approssimazione della distribuzione a posteriori | [28][l28] | TODO |
# | 17      | 2023/04/11 | MCMC e schema beta-binomiale | [29][l29] | TODO |
# | 18      | 2023/04/13 | Diagnostica delle catene markoviane; sintesi a posteriori | [30][l30], [31][l31] | TODO |
# | 19      | 2023/04/17 | **Prova in itinere (1)** | -- | -- |
# | 20      | 2023/04/18 | Predizione bayesiana; inferenza su una media | [32][l32], [33][l33] | TODO  | 
# | 21      | 2023/04/20 | Inferenza su due o più gruppi | [34][l34] | TODO  | <!-- linear regression -->
# | 22      | 2023/04/24 | Il modello di regressione lineare | [35][l35], [36][l36] | TODO  |
# | 23      | 2023/04/27 | Regressione lineare con PyMC; confronto tra le medie di due gruppi | [37][l37], [38][l38] | TODO  | 
# | 24      | 2023/05/02 | PPC  | [39][l39] | TODO  | <!-- inferenza frequentista -->
# | 25      | 2023/05/04 | La distribuzione delle statistiche campionarie | [40][l40], [41][l41] | TODO  |
# | 26      | 2023/05/08 | I test di ipotesi statistiche | [42][l42] | TODO  |
# | 27      | 2023/05/09 | Esercitazione  | -- | TODO  |
# | 28      | 2023/05/11 | I limiti dell'inferenza frequentista | [43][l43], [44][l44] | TODO  |
# | 29      | 2023/05/15 | **Prova in itinere (2)** | -- | -- |
# | 30      | 2023/05/16 | **Presentazione dei progetti di gruppo (1)** | -- | -- |
# | 31      | 2023/05/18 | **Presentazione dei progetti di gruppo (2)** | -- | -- |
# | 32      | 2023/05/22 | **Presentazione dei progetti di gruppo (3)** | -- | -- |
# 
# 
# [l1]: https://ccaudek.github.io/ds4psy_2023/preface.html
# <!--intro to coding -->
# [l2]: https://ccaudek.github.io/ds4psy_2023/010_installation.html
# [l3]: https://ccaudek.github.io/ds4psy_2023/015_intro_python.html
# [l4]: https://ccaudek.github.io/ds4psy_2023/020_intro_numpy.html
# [l5]: https://ccaudek.github.io/ds4psy_2023/025_intro_pandas.html
# [l6]: https://ccaudek.github.io/ds4psy_2023/035_intro_matplotlib.html
# [l7]: https://ccaudek.github.io/ds4psy_2023/040_intro_seaborn.html
# <!-- descriptive stats -->
# [l8]: https://ccaudek.github.io/ds4psy_2023/051_key_notions.html
# [l9]: https://ccaudek.github.io/ds4psy_2023/055_measurement.html
# [l10]: https://ccaudek.github.io/ds4psy_2023/060_freq_distr.html
# [l11]: https://ccaudek.github.io/ds4psy_2023/065_loc_scale.html
# [l12]: https://ccaudek.github.io/ds4psy_2023/070_correlation.html
# <!-- probability theory -->
# [l13]: https://ccaudek.github.io/ds4psy_2023/105_combinatorics.html
# [l14]: https://ccaudek.github.io/ds4psy_2023/110_intro_prob.html
# [l15]: https://ccaudek.github.io/ds4psy_2023/115_conditional_prob.html
# [l16]: https://ccaudek.github.io/ds4psy_2023/120_bayes_theorem.html
# [l17]: https://ccaudek.github.io/ds4psy_2023/125_expval_var.html
# [l18]: https://ccaudek.github.io/ds4psy_2023/130_joint_prob.html
# [l19]: https://ccaudek.github.io/ds4psy_2023/135_density_func.html
# [l20]: https://ccaudek.github.io/ds4psy_2023/205_discr_rv_distr.html
# [l21]: https://ccaudek.github.io/ds4psy_2023/210_cont_rv_distr.html
# [l22]: https://ccaudek.github.io/ds4psy_2023/215_rng.html
# [l23]: https://ccaudek.github.io/ds4psy_2023/225_likelihood.html
# <!-- bayesian inference -->
# [l24]: https://ccaudek.github.io/ds4psy_2023/305_intro_bayes.html
# [l25]: https://ccaudek.github.io/ds4psy_2023/310_subj_prop.html
# [l26]: https://ccaudek.github.io/ds4psy_2023/316_conjugate_families.html
# [l27]: https://ccaudek.github.io/ds4psy_2023/320_balance_prior_post.html
# [l28]: https://ccaudek.github.io/ds4psy_2023/325_metropolis.html
# [l29]: https://ccaudek.github.io/ds4psy_2023/330_beta_binomial.html
# [l30]: https://ccaudek.github.io/ds4psy_2023/335_mcmc_diagnostics.html
# [l31]: https://ccaudek.github.io/ds4psy_2023/340_summarize_posterior.html
# [l32]: https://ccaudek.github.io/ds4psy_2023/345_bayesian_prediction.html
# [l33]: https://ccaudek.github.io/ds4psy_2023/350_normal_normal_mod.html
# [l34]: https://ccaudek.github.io/ds4psy_2023/355_groups_comparison.html
# <!-- linear regression -->
# [l35]: https://ccaudek.github.io/ds4psy_2023/400_reglin_1.html
# [l36]: https://ccaudek.github.io/ds4psy_2023/405_reglin_2.html
# [l37]: https://ccaudek.github.io/ds4psy_2023/410_reglin_3.html
# [l38]: https://ccaudek.github.io/ds4psy_2023/415_reglin_4.html
# [l39]: https://ccaudek.github.io/ds4psy_2023/420_reglin_ppc.html
# <!-- inferenza frequentista -->
# [l40]: https://ccaudek.github.io/ds4psy_2023/500_intro_frequentist.html
# [l41]: https://ccaudek.github.io/ds4psy_2023/505_conf_interv.html
# [l42]: https://ccaudek.github.io/ds4psy_2023/510_test_ipotesi.html
# [l43]: https://ccaudek.github.io/ds4psy_2023/515_limiti_stat_frequentista.html
# [l44]: https://ccaudek.github.io/ds4psy_2023/520_s_m_errors.html
# 
# 
# [hw1]: https://ccaudek.github.io/psicometria_2023/exercises/01_python/ex_python.html
# [hw2]: https://ccaudek.github.io/psicometria_2023/exercises/01_python/ex_pandas.html
# [hw3]: https://ccaudek.github.io/psicometria_2023/exercises/01_python/ex_matplotlib.html
# 
# 
# <!-- ------------------------------------------------------ -->
# 
# ## Settimana 1: 2 marzo 2023
# 
# **Presentazione dell'insegnamento**
# 
# La prima settimana di lezioni include un unico incontro. In tale incontro verranno descritte le attività che si svolgeranno durante il semestre. Nell'ottica dell'_insegnamento capovolto_, questa prima settimana darà il tempo agli studenti di leggere il materiale didattico e di svolgere gli esercizi WarmUp relativi agli argomenti trattati nella seconda settimana di lezione.
# 
# *Homeworks*: 
# 
# Oltre a svolgere gli esercizi WarmUp assegnati:
# 
# - iscriversi alla pagina Moodle dell'insegnamento;
# - installare Anaconda sul proprio computer (si vedano le [istruzioni](https://ccaudek.github.io/ds4psy_2023/010_installation.html));
# - creare l'ambiente virtuale che useremo durante l'insegnamento;
# - installare Visual Studio Code;
# - svolgere gli esercizi WarmUp.
# - Iscriversi ad un gruppo Moodle. 
# 
# <!-- ------------------------------------------------------ -->
# 
# ## Settimana 2: 6 marzo 2023
# 
# **Introduzione a Python**
# 
# Questo è un insegnamento di analisi dei dati, non di programmazione. Però la Data Science richiede sia competenze statistiche che competenze informatiche. Ovviamente non è possibile diventare dei programmatori, in tre mesi.  Ma però è possibile imparare i concetti di base della programmazione (in qualsiasi linguaggio). E, sopratutto, imparare quelli che sono gli strumenti che useremo per le manipolazioni e le analisi dei dati. Ciò che verrà presentato, su Python, NumPy, Pandas, Matplotlib e Seaborn *non* va memorizzato. Questo tipo di cose si imparano usandole. L'obiettivo è invece capire come utilizzare gli strumenti che verranno presentati, così da saperli poi usare nel contesto sia dell'esame sia del lavoro di gruppo che porterà a replicare un'analisi di dati presentata in una recente pubblicazione psicologica.
# 
# 
# <!-- ------------------------------------------------------ -->
# 
# ## Settimana 3: 13 marzo 2023
# 
# **Statistica descrittiva e analisi esplorativa dei dati**
# 
# La statistica descrittiva è estremamente semplice e molti di voi conoscono già quasi tutti i concetti che discuteremo (media, mediana, varianza, ...). Le (poche) formule che sono presenti nella dispensa **vanno memorizzate** e le (poche) dimostrazioni algebriche discusse **vanno capite** (questo è un utile esercizio propedeutico alle dimostrazioni più complesse che incontreremo in seguito). Una volta capiti i concetti teorici della statistica descrittiva, si pone poi un secondo problema importante: quello di eseguire in pratica l'analisi dei dati. Non basta avere capito la formula della deviazione standard, ad esempio: se avete un file di dati, dovete sapere come applicare questa conoscenza utilizzando un software. È chiaro che, nel caso di dati psicologici reali, siamo sempre di fronte ad un enorme numero di osservazioni. Dunque, nessuna manipolazione di questi dati può essere fatta con metodi "carta e penna". Pertanto è necessario sapere usare un software (nel nostro caso, Python) per svolgere quella che si chiama l'analisi esplorativa dei dati. Lo studio delle procedure dell'analisi esplorativa dei dati mediante software è un progetto a lungo termine: il software cambia continuamente e i problemi di analisi dei dati che ci poniamo sono sempre diversi, variano da progetto a progetto. Il nostro obiettivo qui è quello di imparare le tecniche di base: importare i dati, calcolare le statistiche descrittive, svolgere semplici manipolazioni di dati (per esempio, la creazione di nuove variabili), visualizzare le distribuzioni e le relazioni tra variabili. Impareremo anche ad usare i notebook Jupyter con cui creare un report che contiene sia testo (in formato markdown) sia il codice Python.
# 
# 
# <!-- ------------------------------------------------------ -->
# 
# ## Settimana 4: 20 marzo 2023
# 
# **Elementi di teoria delle probabilità**
# 
# La teoria delle probabilità è senza dubbio un argomento complesso, che richiede uno studio prolungato. I problemi della teoria delle probabilità, ad esempio quelli che richiedono il calcolo combinatorio, sono tra i problemi più complessi che si possono immaginare. I matematici più geniali della storia dell'umanità hanno compiuto numerosi errori nel tentativo di risolvere problemi di questo tipo. Il nostro obiettivo qui non è quello di sviluppare una conoscenza approfondita dei temi del calcolo delle probabilità, ma solo quello di capire i concetti necessari all'analisi dei dati psicologici. Quindi, gli esercizi di calcolo delle probabilità che incontreremo saranno (relativamente) semplici, dato che avranno quale unico obiettivo quello di chiarire il significato dei concetti di base della teoria delle probabilità. Tali concetti (ad esempio, la densità di probabilità, il valore atteso, la probabilità congiunta, condizionata, marginale, ecc.) ci forniscono il linguaggio nei termini del quale si articola la discussione dell'inferenza statistica. Ma quanta matematica / teoria delle probabilità dobbiamo conoscere per il nostro scopo, ovvero l'analisi dei dati psicologici? Una possibile risposta è stata fornita da Andrew Gelman: _"If you wanted to do foundational research in statistics in the mid-twentieth century, you had to be a bit of a mathematician, whether you wanted to or not ... if you want to do statistical research at the turn of the twenty-first century, you have to be a computer programmer."_ In altre parole, per un corso di base come il presente non è necessaria una conoscenza approfondita dell'analisi matematica e della teoria delle probabilità; molto più utili sono invece le competenze computazionali e algoritimiche. Dunque, selezioneremo dalla teoria delle probabilità solo quegli strumenti che risultano utili (e necessari) ai nostri scopi, senza approfondire. Invece, utilizzeremo fin dall'inizio un approccio algoritmico perché, anche nella trattazione degli argomenti successivi, questo approccio ci consentirà di sostituire problemi matematici (complessi) con problemi algoritmici (molto più semplici da affrontare).
# 
# <!-- ------------------------------------------------------ -->
# 
# 
