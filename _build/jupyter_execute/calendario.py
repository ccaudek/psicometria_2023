#!/usr/bin/env python
# coding: utf-8

# # Calendario 
# 
# Viene qui fornito un calendario delle attività didattiche: l'elenco dei materiali da studiare ogni settimana, le scadenze per la consegna degli esercizi WarmUp, e le scadenze dei report in itinere. I compiti WarmUp assegnati vanno svolti **prima** di ciascuna lezione lezione. 
# 
# ```{warning}
# Il presente calendario **non è definitivo** e potrà subire delle variazioni che saranno comunicate mediante l'aggiornamento di questa pagina web.
# ```
# 
# ## Calendario
# 
# | Lezione | Data       | Argomento                                  | Materiali didattici     | Homework                 |
# | ------- | ---------- | ------------------------------------------ | ----------------------- | ------------------------ |
# | 1       | 2023/03/02 | Presentazione                       | [1][l1], [2][l2]  | -- | <!--intro to coding -->
# | 2       | 2023/03/06 | Introduzione a Python               | [3][l3]          | [hw1][hw1] |
# | 3       | 2023/03/07 | Introduzione a Numpy e Pandas       | [4][l4], [5][l5] | [hw2][hw1] |
# | 4       | 2023/03/09 | Introduzione a Matlibplot e Seaborn | [6][l6], [7][l7] | [hw3][hw1] | <!-- descriptive stats -->
# | 5       | 2023/03/13 | La misurazione in psicologia; dati e frequenze | [8][l8], [9][l9], [10][l10] | [hw4][hw1] |
# | 6       | 2023/03/14 | Statistica descrittiva: indici di posizione e di scala; | [11][l11]  | [hw5][hw1] |
# | 7       | 2023/03/16 | Le relazioni tra variabili  | [12][l12]  | TODO | <!-- probability theory -->
# | 8       | 2023/03/20 | Calcolo combinatorio; probabilità condizionata | [13][l13], [14][l14], [15][l15] | [hw1][hw1] |
# | 9       | 2023/03/21 | Teorema di Bayes | [16][l16] | [hw1][hw1] |
# | 10       | 2023/03/23 | Variabili casuali, probabilità congiunta, densità di probabilità | [17][l17], [18][l18], [19][l19] | [hw1][hw1] |
# | 11       | 2023/03/27 | Distribuzioni di massa di probabilità     | [20][l20]    | TODO |
# | 12       | 2023/03/28 | Distribuzioni di densità di probabilità   | [21][l21]    | TODO |
# | 13      | 2023/03/30 | La funzione di verosimiglianza | [22][l22], [23][l23] | TODO |<!-- bayesiann inference -->
# | 14      | 2023/04/03 | Pensare alla probabilità in termini bayesiani | [24][l24], [25][l25] | TODO               |
# | 15      | 2023/04/04 | Distribuzioni coniugate; influenza della distribuzione a priori | [26][l26], [27][l27] | TODO |
# | 16      | 2023/04/06 | Approssimazione della distribuzione a posteriori | [28][l28] | TODO |
# | 17      | 2023/04/11 | MCMC e schema beta-binomiale | [29][l29] | TODO |
# | 18      | 2023/04/13 | Diagnostica delle catene markoviane; sintesi a posteriori | [30][l30], [31][l31] | TODO |
# | 19      | 2023/04/17 | **Prova in itinere (1)** | -- | -- |
# | 20      | 2023/04/18 | Predizione bayesiana; inferenza su una media | [32][l32], [33][l33] | TODO  | 
# | 21      | 2023/04/20 | Inferenza su due o più gruppi | [34][l34] | TODO  | <!-- linear regression -->
# | 22      | 2023/04/24 | Il modello di regressione lineare | [35][l35], [36][l36] | TODO  |
# | 23      | 2023/04/27 | Regressione lineare con PyMC; confronto tra le medie di due gruppi | [37][l37], [38][l38] | TODO  | 
# | 24      | 2023/05/02 | PPC  | [39][l39] | TODO  | <!-- inferenza frequentista -->
# | 25      | 2023/05/04 | La distribuzione delle statistiche campionarie | [40][l40], [41][l41] | TODO  |
# | 26      | 2023/05/08 | I test di ipotesi statistiche | [42][l42] | TODO  |
# | 27      | 2023/05/09 | Esercitazione  | -- | TODO  |
# | 28      | 2023/05/11 | I limiti dell'inferenza frequentista | [43][l43], [44][l44] | TODO  |
# | 29      | 2023/05/15 | **Prova in itinere (2)** | -- | -- |
# | 30      | 2023/05/16 | **Presentazione dei progetti di gruppo (1)** | -- | -- |
# | 31      | 2023/05/18 | **Presentazione dei progetti di gruppo (2)** | -- | -- |
# | 32      | 2023/05/22 | **Presentazione dei progetti di gruppo (3)** | -- | -- |
# 
# 
# [l1]: https://ccaudek.github.io/ds4psy_2023/preface.html
# <!--intro to coding -->
# [l2]: https://ccaudek.github.io/ds4psy_2023/010_installation.html
# [l3]: https://ccaudek.github.io/ds4psy_2023/015_intro_python.html
# [l4]: https://ccaudek.github.io/ds4psy_2023/020_intro_numpy.html
# [l5]: https://ccaudek.github.io/ds4psy_2023/025_intro_pandas.html
# [l6]: https://ccaudek.github.io/ds4psy_2023/035_intro_matplotlib.html
# [l7]: https://ccaudek.github.io/ds4psy_2023/040_intro_seaborn.html
# <!-- descriptive stats -->
# [l8]: https://ccaudek.github.io/ds4psy_2023/051_key_notions.html
# [l9]: https://ccaudek.github.io/ds4psy_2023/055_measurement.html
# [l10]: https://ccaudek.github.io/ds4psy_2023/060_freq_distr.html
# [l11]: https://ccaudek.github.io/ds4psy_2023/065_loc_scale.html
# [l12]: https://ccaudek.github.io/ds4psy_2023/070_correlation.html
# <!-- probability theory -->
# [l13]: https://ccaudek.github.io/ds4psy_2023/105_combinatorics.html
# [l14]: https://ccaudek.github.io/ds4psy_2023/110_intro_prob.html
# [l15]: https://ccaudek.github.io/ds4psy_2023/115_conditional_prob.html
# [l16]: https://ccaudek.github.io/ds4psy_2023/120_bayes_theorem.html
# [l17]: https://ccaudek.github.io/ds4psy_2023/125_expval_var.html
# [l18]: https://ccaudek.github.io/ds4psy_2023/130_joint_prob.html
# [l19]: https://ccaudek.github.io/ds4psy_2023/135_density_func.html
# [l20]: https://ccaudek.github.io/ds4psy_2023/205_discr_rv_distr.html
# [l21]: https://ccaudek.github.io/ds4psy_2023/210_cont_rv_distr.html
# [l22]: https://ccaudek.github.io/ds4psy_2023/215_rng.html
# [l23]: https://ccaudek.github.io/ds4psy_2023/225_likelihood.html
# <!-- bayesian inference -->
# [l24]: https://ccaudek.github.io/ds4psy_2023/305_intro_bayes.html
# [l25]: https://ccaudek.github.io/ds4psy_2023/310_subj_prop.html
# [l26]: https://ccaudek.github.io/ds4psy_2023/316_conjugate_families.html
# [l27]: https://ccaudek.github.io/ds4psy_2023/320_balance_prior_post.html
# [l28]: https://ccaudek.github.io/ds4psy_2023/325_metropolis.html
# [l29]: https://ccaudek.github.io/ds4psy_2023/330_beta_binomial.html
# [l30]: https://ccaudek.github.io/ds4psy_2023/335_mcmc_diagnostics.html
# [l31]: https://ccaudek.github.io/ds4psy_2023/340_summarize_posterior.html
# [l32]: https://ccaudek.github.io/ds4psy_2023/345_bayesian_prediction.html
# [l33]: https://ccaudek.github.io/ds4psy_2023/350_normal_normal_mod.html
# [l34]: https://ccaudek.github.io/ds4psy_2023/355_groups_comparison.html
# <!-- linear regression -->
# [l35]: https://ccaudek.github.io/ds4psy_2023/400_reglin_1.html
# [l36]: https://ccaudek.github.io/ds4psy_2023/405_reglin_2.html
# [l37]: https://ccaudek.github.io/ds4psy_2023/410_reglin_3.html
# [l38]: https://ccaudek.github.io/ds4psy_2023/415_reglin_4.html
# [l39]: https://ccaudek.github.io/ds4psy_2023/420_reglin_ppc.html
# <!-- inferenza frequentista -->
# [l40]: https://ccaudek.github.io/ds4psy_2023/500_intro_frequentist.html
# [l41]: https://ccaudek.github.io/ds4psy_2023/505_conf_interv.html
# [l42]: https://ccaudek.github.io/ds4psy_2023/510_test_ipotesi.html
# [l43]: https://ccaudek.github.io/ds4psy_2023/515_limiti_stat_frequentista.html
# [l44]: https://ccaudek.github.io/ds4psy_2023/520_s_m_errors.html
# 
# 
# ## Assignments and Evaluation
# 
# The Course Grade is based on the following:
# 
# - Seven [Assignments](#assignment-scoring): 95%
#   - [Homework 0][hw0]: 14%
#   - [Homework 1][hw1]: 14%
#   - [Homework 2][hw2]: 14%
#   - [Homework 3][hw3]: 14%
#   - [Homework 4][hw4]: 7%
#   - [Homework 5 proposal][hw5_prop]: 7%
#   - [Homework 5][hw5]: 20%
#   - [Homework 5][hw5] peer grading: 5%
# - [Between-Class Participation](#participation): 5%
# 
# [hw1]: https://ccaudek.github.io/psicometria_2023/exercises/01_python/ex_python.html
# [hw2]: https://python-public-policy.afeld.me/en/{{school_slug}}/hw_2.html
# [hw3]: https://python-public-policy.afeld.me/en/{{school_slug}}/hw_3.html
# [hw4]: https://python-public-policy.afeld.me/en/{{school_slug}}/hw_4.html
# [hw5_prop]: https://python-public-policy.afeld.me/en/{{school_slug}}/hw_4.html#homework-6-proposal
# [hw5]: https://python-public-policy.afeld.me/en/{{school_slug}}/hw_5.html
# 
# 
# <!-- ------------------------------------------------------ -->
# 
# ## Settimana 1: 2 marzo 2023
# 
# **Presentazione dell'insegnamento**
# 
# La prima settimana di lezioni include un unico incontro. In tale incontro verranno descritte le attività che si svolgeranno durante il semestre. Nell'ottica dell'_insegnamento capovolto_, questa prima settimana darà il tempo agli studenti di leggere il materiale didattico e di svolgere gli esercizi WarmUp relativi agli argomenti trattati nella seconda settimana di lezione.
# 
# *Homeworks*: 
# 
# Oltre a svolgere gli esercizi WarmUp assegnati:
# 
# - iscriversi alla pagina Moodle dell'insegnamento;
# - installare Anaconda sul proprio computer (si vedano le [istruzioni](https://ccaudek.github.io/ds4psy_2023/010_installation.html));
# - creare l'ambiente virtuale che useremo durante l'insegnamento;
# - installare Visual Studio Code;
# - svolgere gli esercizi WarmUp.
# 
# 
# - Iscriversi ad un gruppo Moodle. 
# 
# <!-- ------------------------------------------------------ -->
# 
# **Introduzione a Python**
# 
# Questo è un insegnamento di analisi dei dati, non di programmazione. Però la Data Science richiede sia competenze statistiche che competenze informatiche. Ovviamente non è possibile diventare dei programmatori, in tre mesi.  Ma però è possibile imparare i concetti di base della programmazione (in qualsiasi linguaggio). E, sopratutto, imparare quelli che sono gli strumenti che useremo per le manipolazioni e le analisi dei dati. Ciò che verrà presentato, su Python, NumPy, Pandas, Matplotlib e Seaborn *non* va memorizzato. Questo tipo di cose si imparano usandole. L'obiettivo è invece capire come utilizzare gli strumenti che verranno presentati, così da saperli poi usare nel contesto sia dell'esame sia del lavoro di gruppo che porterà a replicare un'analisi di dati presentata in una recente pubblicazione psicologica.
# 
# 
# <!-- ------------------------------------------------------ -->
# 
# **Statistica descrittiva e analisi esplorativa dei dati**
# 
# La statistica descrittiva è estremamente semplice e molti di voi conoscono già quasi tutti i concetti che discuteremo (media, mediana, varianza, ...). Le (poche) formule che sono presenti nella dispensa **vanno memorizzate** e le (poche) dimostrazioni algebriche discusse **vanno capite** (questo è un utile esercizio propedeutico alle dimostrazioni più complesse che incontreremo in seguito). Una volta capiti i concetti teorici della statistica descrittiva, si pone poi un secondo problema importante: quello di eseguire in pratica l'analisi dei dati. Non basta avere capito la formula della deviazione standard, ad esempio: se avete un file di dati, dovete sapere come applicare questa conoscenza utilizzando un software. È chiaro che, nel caso di dati psicologici reali, siamo sempre di fronte ad un enorme numero di osservazioni. Dunque, nessuna manipolazione di questi dati può essere fatta con metodi "carta e penna". Pertanto è necessario sapere usare un software (nel nostro caso, Python) per svolgere quella che si chiama l'analisi esplorativa dei dati. Lo studio delle procedure dell'analisi esplorativa dei dati mediante software è un progetto a lungo termine: il software cambia continuamente e i problemi di analisi dei dati che ci poniamo sono sempre diversi, variano da progetto a progetto. Il nostro obiettivo qui è quello di imparare le tecniche di base: importare i dati, calcolare le statistiche descrittive, svolgere semplici manipolazioni di dati (per esempio, la creazione di nuove variabili), visualizzare le distribuzioni e le relazioni tra variabili. Impareremo anche ad usare i notebook Jupyter con cui creare un report che contiene sia testo (in formato markdown) sia il codice Python.
# 
# *Incontri di lezione ed esercitazioni pratiche #* 2, 3, 4
# 
# *Letture per la settimana successiva:*  
# 
# - DSpP capitoli 6, 7, 8, 9.
# - [R for Data Science](https://r4ds.had.co.nz/r-markdown.html) capitoli 7, 8, 16, 27, 29, 30.
# 
# *Compiti*: 
# 
# - Esercizi WarmUp 
# - Replicare sul proprio computer il seguente [demo](http://htmlpreview.github.io/?https://github.com/ccaudek/ds4psy_R_demos/blob/main/demos_ch07/demo07_01.html).
# 
# *Risorse aggiuntive*
# 
# - [Introduction to Modern Statistics](https://openintro-ims.netlify.app) capitoli 4, 5.
# 
# <!-- ------------------------------------------------------ -->
# 
# ## `r meeting_headers[3]`
# 
# **Elementi di teoria delle probabilità**
# 
# La teoria delle probabilità è senza dubbio un argomento complesso, che richiede uno studio molto lungo e l'introduzione di tanti concetti. I problemi della teoria delle probabilità, ad esempio quelli che richiedono il calcolo combinatorio, sono tra i problemi più complessi che si possono immaginare. I matematici più geniali della storia dell'umanità hanno compiuto numerosi errori nel tentativo di risolvere problemi di questo tipo. Il nostro obiettivo qui non è quello di sviluppare una conoscenza più o meno approfondita dei temi del calcolo delle probabilità, ma solo quello di capire i concetti necessari all'analisi dei dati psicologici. Quindi, gli esercizi di calcolo delle probabilità che incontreremo saranno (relativamente) semplici, dato che avranno quale unico obiettivo quello di chiarire il significato delle nozioni di base della teoria delle probabilità. Tali concetti (ad esempio, la densità di probabilità, il valore atteso, la probabilità congiunta, condizionata, marginale, ecc.) ci forniscono il linguaggio nei termini del quale si articola la discussione dell'inferenza statistica. Ma quanta matematica / teoria delle probabilità dobbiamo conoscere per il nostro scopo, ovvero l'analisi dei dati psicologici? Una possibile risposta è stata fornita da Andrew Gelman: _"If you wanted to do foundational research in statistics in the mid-twentieth century, you had to be a bit of a mathematician, whether you wanted to or not ... if you want to do statistical research at the turn of the twenty-first century, you have to be a computer programmer."_ In altre parole, non è necessaria una conoscenza molto approfondita dell'analisi matematica e della teoria delle probabilità; molto più utili sono invece le competenze computazionali e algoritimiche. Dunque, selezioneremo dalla teoria delle probabilità solo quegli strumenti che risultano utili (e necessari) ai nostri scopi, senza troppi approfondimenti. Invece, utilizzeremo da subito un approccio algoritmico, perché questo sarà il metodo che verrà impiegato nella trattazione degli argomenti successivi.
# 
# *Incontri di lezione ed esercitazioni pratiche #* 5, 6, 7
# 
# *Letture per la settimana successiva:*  
# 
#  - DSpP capitoli 10, 11, 12, 13
# 
# *Compiti*: 
# 
# - **Relazione in itinere 1:** 
#   - leggere l'articolo di riferimento;
#   - importare i dati degli autori in $\textsf{R}$;
#   - creare un un file RMarkdown che include alcune statistiche descrittive sui dati forniti dagli autori dell'articolo scelto.
# 
# 
# *Risorse aggiuntive*
# 
# - Espresso in forma sintetica, tutto ciò che c'è da sapere sugli argomenti discussi questa settimana è presentato in questa [pagina web](https://ermongroup.github.io/cs228-notes/preliminaries/probabilityreview/).
# - I materiali di [ Introduction to Probability and Statistics](https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/) sono fatti benissimo (qualche anno fa, questo è stato giudicato come il migliore insegnamento undergraduate del MIT!). La trattazione degli argomenti di teoria delle probabilità è molto simile a quella della dispensa. Noi approfondiremo un po' di più i temi dell'inferenza bayesiana -- invece trascureremo quasi del tutto l'approccio frequentista (che reputo obsoleto).
# 
# <!-- ------------------------------------------------------ -->
# 
# ## `r meeting_headers[4]`
# 
# **Distribuzioni di densità e modellizzazione bayesiana**
# 
# Questa settimana introdurremo le distribuzioni di densità di probabilità. Tra esse, la più importante è certamente la gaussiana. Ma non c'è molto da capire sulla gaussiana: c'è una formula che definisce una famiglia di curve che hanno alcune proprietà. Impareremo ad usare $\textsf{R}$ per disegnare una tale curva e per calcolare alcune caratteristiche di essa, come ad esempio l'area sottesa alla curva in un intervallo. Non c'è niente di particolarmente difficile in tutto questo.  Nell'appendice della dispensa è fornita una breve discussione della relazione tra distribuzione gaussiana e il metodo dei minimi quadrati. Quella discussione illustra l'eleganza della teoria della probabilità e suggerisce come, dietro la semplicità apparente della trattazione nella dispensa, c'è in realtà molto di più di quello che appare a primo sguardo. Ma questi temi non fanno parte del programma d'esame: li presento solo come una curiosità.  Un maggiore sforzo andrà invece diretto alla distribuzione Beta (e alle funzioni beta e gamma). Non perché la distribuzione Beta sia molto importante (la gaussiana lo è enormemente di più), ma perché questa distribuzione ci consentirà di capire come ottenere _per via analitica_ la distribuzione a posteriori quando i dati sono costituiti da una proporzione. La distribuzione Beta ci consentirà anche, in una particolare circostanza, di risolvere in forma analitica (e sarà l'unico caso di questo tipo che discuteremo) l'integrale che si trova a denominatore nel rapporto di Bayes. Per ottenere questi risultati è però necessario conoscere nei dettagli le proprietà della distribuzione Beta. Per cui ad essa dovrà essere rivolta un'attenzione particolare. Questa settimana verrà anche introdotta la funzione di verosimiglianza. Questo concetto non presenta alcuna difficoltà. Il modo più semplice per capire cosa significa "verosimiglianza" è quello di fare una simulazione. Dunque a questo punto del semestre gli studenti dovranno avere capito e dovranno sapere usare il ciclo `for` e l'aritmetica vettorializzata in $\textsf{R}$. 
# 
# *Incontri di lezione ed esercitazioni pratiche #* 8, 9, 10
# 
# *Letture per la settimana successiva:*  
# 
#  - DSpP capitoli 14, 15, 16, 17
# 
# *Compiti*: 
# 
# - Installazione di `cmdstanr`.
# 
# *Risorse aggiuntive*
# 
# - Shiny app per la [distribuzione beta](https://github.com/ccaudek/ds4psy_R_demos/blob/main/demos_ch13/beta_distr.R).
# - Le video-registrazioni di Richard McElreath basate sul suo famoso libro [Statistical rethinking](https://github.com/rmcelreath/stat_rethinking_2022) forniscono sicuramente la migliore introduzione alla statistica bayesiana. Il libro (ora nella seconda edizione) ha avuto uno straordinario successo. Non l'ho adottato perché è ad un livello troppo avanzato per un primo insegnamento di statistica per psicologi. Tuttavia, la parte iniziale del libro copre gli stessi argomenti che discutiamo anche noi: fortemente consigliata! 
# 
# <!-- ------------------------------------------------------ -->
# 
# ## `r meeting_headers[5]`
# 
# **Distribuzioni a priori coniugate; l'influenza della distribuzione a priori**
# 
# L'inferenza bayesiana richiede la soluzione di un'equazione che, a denominatore, contiene un integrale che, in generale, non è risolvibile per via analitica. Un caso a parte è quello nel quale la verosimiglianza viene combinata con una distribuzione a priori tale per cui la distribuzione a posteriori è della stessa famiglia della distribuzione a priori. Esamineremo qui il caso più semplice, ovvero lo schema beta-binomiale. Ci porremo anche il problema di capire qual è l'effetto della distribuzione a priori sulla distribuzione a posteriori. Dato che la distribuzione a priori è arbitraria, l'influenza della distribuzione a priori sulla distribuzione a posteriori rappresenta l'aspetto più controverso della stastistica bayesiana. Ma le cose stanno veramente così? L'arbitrarietà della distribuzione a priori rappresenta il limite maggiore della statistica bayesiana? Oppure può essere interpretato come uno dei punti di forza più grandi di questo approccio?
# 
# *Incontri di lezione ed esercitazioni pratiche #* 11, 12, 13
# 
# *Letture per la settimana successiva:*  
# 
#  - DSpP capitoli 18, 19, 20
# 
# *Compiti*: 
# 
# *Risorse aggiuntive*
# 
# - [Bayes Rules!](https://www.bayesrulesbook.com/chapter-3.html) capitolo 3.
# 
# <!-- ------------------------------------------------------ -->
# 
# ## `r meeting_headers[6]`
# 
# **Stima della distribuzione a posteriori**
# 
# In generale, la distribuzione a posteriori non può essere determinata per via analitica, ma solo per via numerica. Questa settimana ci porremo la seguente domanda: come è possibile approssimare numericamente la forma di una distribuzione _che non conosciamo_? Questo sembra magico: è possibile descrivere in maniera numerica, con un livello arbitrario di precisione, la forma di una distribuzione sconosciuta. Esamineremo nei dettagli due metodi che rendono "meno magica" la soluzione a questo problema: i metodi basati su griglia e i metodi di campionamento Monte Carlo basati su Catena di Markov. Ci concentreremo in particolare sul primo algoritmo che è stato usato per risolvere questo problema, ovvero l'_algoritmo di Metropolis_.
# 
# *Incontri di lezione ed esercitazioni pratiche #* 14, 15, 16
# 
# *Letture per la settimana successiva:*  
# 
#  - DSpP capitoli 
# 
# *Compiti*: 
# 
# - **Relazione in itinere 2:** completare le analisi descrittivie e replicare le analisi frequentiste svolte dagli autori;
# 
# 
# <!-- ------------------------------------------------------ -->
# 
# ## `r meeting_headers[7]`
# 
# *Incontri di lezione ed esercitazioni pratiche #* 17
# 
# - **Primo parziale**
# 
# *Letture per la settimana successiva:*  
# 
#  - DSpP capitoli 21, 22, 23
# 
# <!-- ------------------------------------------------------ -->
# 
# ## `r meeting_headers[8]`
# 
# **Introduzione al linguaggio Stan; inferenza su una proporzione**
# 
# Gli algoritmi MCMC forniscono una risposta ai problemi dell'inferenza bayesiana. Sono state proposte varie implementazioni di algoritmi che consentono di ottenere una sequenza di osservazioni che approssima da la distribuzione di probabilità a posteriori. Tali algoritmi sono più efficienti dell'algoritmo di Metropolis che abbiamo discusso. Tuttavia, quello dei campionatori è un aspetto estremamente tecnico che non verrà qui affrontato. Invece, ci porremo il problema di imparare ad usare un linguaggio probabilistico che consente di fare utilizzare uno di questi campionatori sul nostro computer. Il linguaggio probabilistico che verrà qui presentato si chiama Stan. Quale campionatore, Stan utilizza il formalismo di Hamilton, ovvero il più efficiente dei campionatori che sono stati finora proposti.
# 
# *Incontri di lezione ed esercitazioni pratiche #* 18, 19, 20
# 
# *Letture per la settimana successiva:*  
# 
#  - DSpP capitoli 24, 25, 26
# 
# *Compiti*: 
# 
# <!-- ------------------------------------------------------ -->
# 
# ## `r meeting_headers[9]`
# 
# **Il modello statistico lineare (I)**
# 
# Il modello statistico lineare costituisce il fondamento di analisi statiche più complesse e, nella sua forme di base, consente di affrontare molti problemi statistici. Qui ci focalizzeremo sul confronto tra due gruppi indipendenti e sul caso del confronto tra più di due gruppi. Questo è il caso di una variabile dipendente continua e di una variabile indipendente qualitativa con due o più modalità. Il caso di una variabile dipendente continua e di una sola variabile indipendente continua costituisce il modello lineare classico. La presenza di due o più predittori continui definisce il modello di regressione multipla, mentre il caso di due o più variabili dipendenti continue definisce il caso della regressione multivariata. Nella presente trattazione faremo solo qualche accenno alla regressione multipla e non considereremo la regressione multivariata, mentre esamineremo tutti gli altri casi elencati sopra.
# 
# *Incontri di lezione ed esercitazioni pratiche #* 21, 22, 23
# 
# *Letture per la settimana successiva:*  
# 
#  - DSpP capitoli 27, 28, 29, 30
# 
# *Compiti*: 
# 
# 
# <!-- ------------------------------------------------------ -->
# 
# ## `r meeting_headers[10]`
# 
# **Il modello statistico lineare (II)**
# 
# Questa settimana presenteremo un'estensione al modello lineare che impiega una o più variabili indipendenti qualitative, con due o più modalità.  Il caso di una o più variabili indipendenti qualitative con più di due modalità definisce il cosiddetto modello ANOVA. Un'altra estensione del modello lineare classico, estremamente popolare in psicologia, è il modello gerarchico.
# 
# *Incontri di lezione ed esercitazioni pratiche #* 24, 25, 26
# 
# *Letture per la settimana successiva:*  
# 
#  - DSpP capitoli 31, 32, 33, 34
# 
# *Compiti*: 
# 
# - **Relazione in itinere 3:** impostare l'analisi bayesiana usando i dati forniti dagli autori;
# 
# *Risorse aggiuntive*
# 
# - Interessante discussione su [come NON fare una ricerca psicologica](https://statmodeling.stat.columbia.edu/2021/06/16/wow-just-wow-if-you-think-psychological-science-as-bad-in-the-2010-2015-era-you-cant-imagine-how-bad-it-was-back-in-1999/).
# 
# <!-- ------------------------------------------------------ -->
# 
# ## `r meeting_headers[11]`
# 
# **Il confronto tra modelli e selezione di variabili**
# 
# Gli ultimi argomenti che affronteremo sono quelli del contronto tra modelli e della selezione di variabili. Questi argomenti stanno al centro del dibattito contemporaneo. Sono argomenti molto complessi. La soluzione di questi problemi tramite software è semplice. Meno semplice è l'interpretazione dei risultati ottenuti. Lo scopo della discussione di questa settimana è di fornire un'introduzione a tali argomenti in maniera tale da consentire un'interpretazione dei risultati ottenuti tramite software. Di particolare importanza è il concetto di divergenza di Kullback–Leibler. 
# 
# *Incontri di lezione ed esercitazioni pratiche #* 27, 28, 29
# 
# *Compiti*: 
# 
# - Consegna del progetto di gruppo
# 
# <!-- ------------------------------------------------------ -->
# 
# ### `r meeting_headers[12]`
# 
# *Incontri di lezione ed esercitazioni pratiche #* 30, 31, 32
# 
# - **Secondo parziale**
# - **Presentazione dei progetti dei gruppi**
# 
# <!-- ------------------------------------------------------ -->
# 
